\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}

En este apartado se van a comentar los aspectos más interesantes o que han
influido en el desarrollo. Por lo general, estos aspectos vendrán acompañados de
tomas de decisiones que se tuvieron que hacer, se argumentará y explicará el
desarrollo final de estos aspectos.

\section{Elección del proyecto}

La idea de este proyecto no fue propia, ambos tutores tenían en mente realizar
una aplicación enfocada a la docencia de algoritmos de aprendizaje
semi-supervisado. Durante los meses anteriores aparecieron diversos proyectos de
muy diferente índole que podía realizar. La realidad es que la inteligencia
artificial es un campo que me atrae mucho y que incluso me abre la mente a
proyectos personales. Dentro de la oferta de proyectos, el que iba en la línea
de lo que quería hacer era este. Al principio era un poco reacio a la idea de
realizar una aplicación tanto web como de escritorio. Demasiado enfocado en
ello, quería aplicar los algoritmos a algo concreto. Sin embargo, pensándolo
bien, el hecho de poder aprender algoritmos (e incluso la metodología que su
desarrollo conlleva de investigación y entendimiento) pero además, poder
desarrollar habilidades en web (o escritorio, aunque finalmente no ha sido así),
fue la clave para decantarme por el proyecto. Además, dado que disfruto mucho
<<cacharreando>>, los tutores también me comentaron que este proyecto podría
tenerme entretenido muchas horas para desarrollar ambas partes. Con todo ello,
parecía muy interesante y muy provechoso para mi aprendizaje elegir este
proyecto.

\section{Versión de Python}

Al comienzo del proyecto, se valoró la versión de Python en la que realizarlo.
En un principio parecía importante abarcar el mayor número de equipos en los que
el proyecto podía ser instalado y se pensó en alguna versión desde la 3.7. Sin
embargo, pese a que en la biblioteca de algoritmos que se iba a programar sí que
podía tener sentido aumentar los posibles usuarios, la realidad es que el
objetivo del final proyecto es una aplicación web completamente nueva. Se supone
además que, simulando un entorno completamente real y/o empresarial, el equipo
podría tener a su disposición servidor/es propio (o por lo menos,
configurables). En este sentido, dado que el usuario solo necesita acceder a la
Web, no tiene por qué conocer ni tener instalada una versión u otra. Por todo
esto, se pensó en utilizar versiones más recientes. 

En el momento de inicio del proyecto, la más reciente era la 3.10, esto también
es una ventaja en el medio plazo debido a que el periodo de actualizaciones y de
soporte para esta versión termina a finales de 2026 (mientras que algunas
anteriores finalizan en 2024 o incluso 2023).

\section{Utilidades para los algoritmos}

Antes de realizar la aplicación web, pero previendo lo que podía encontrarse.
Surgieron dos grandes problemas, el tratamiento de los datos que utilizan los
algoritmos (etiquetados, no etiquetados, particiones...) y por otro lado, cómo
ajustar estos datos y comunicarlos a la Web cuando los requiera.

No resultaba correcto vincular todos estos pasos en el código de los algoritmos,
ni tampoco en el de la propia aplicación. Tenía mucho más sentido crear unas
utilidades que actuasen de intermediarias en ciertos pasos del procedimiento.

Para el primer problema se crearon tres utilidades principales: 

Un <<particionador>> de datos que se encargara de dividir los datos en conjunto
de entrenamiento y test. Pero que además, si el conjunto estaba pensado para
aprendizaje supervisado, generase aleatoriamente datos no etiquetados.

Un codificador de etiquetas para transformar las etiquetas nominales en
numéricas (necesarias para los algoritmos). De base, el codificador de etiquetas
que propone SKlearn podría ser suficiente. Sin embargo, era necesario por un
lado no tratar los datos no etiquetados (internamente tratados como -1s) y por
el otro devolver de alguna forma las transformaciones realizadas Es decir, a qué
clase nominal corresponde cada número codificado.

Y por último, un cargador de conjuntos de datos (<<datasetloader>>) que
automatizara toda la lectura de un fichero ARFF o CSV, conversión a DataFrame y
las transformaciones de etiquetas (utilizando la utilidad anterior). Y que
además devuelva los datos de los atributos ($\mathbf{x}$) y por separado las
etiquetas ($y$). Es decir, un cargador cuyo resultado pueda ser introducido en los
algoritmos. 

Sobre el segundo problema (datos válidos para la aplicación web) se creó una
utilidad encargada de transformar el conjunto de datos a dos dimensiones. Esto
es, transformar los atributos (que pueden ser más de dos) a exactamente dos,
para poder representarlo en dos dimensiones. Sin embargo, cuando se avanzó un
poco en el desarrollo, era obvio que no interesaba modificar el conjunto de
datos, sino el resultado de la ejecución de la aplicación web. Fue cuando se
implementó y visualizó Self-Training cuando se pudo ver esta casuística.
Finalmente, lo que se hace es: transformar la estructura de datos que retorna la
ejecución de los algoritmos (consultar anexo D) que incluye todos los datos (y
todos sus atributos), a dos dimensiones (solo los atributos, no el <<target>> ni
el resto de columnas). El usuario puede seleccionar dos posibilidades, o
realizar PCA o seleccionar él mismo dos atributos del conjunto.

En ambos casos apareció la decisión de la estandarización. Al principio se
realizaba en ambos casos directamente, pero finalmente se permitía al usuario
elegir si estandarizar o no.

Como se puede ver, no se está normalizando sino estandarizando. Esta decisión es
debida, principalmente, al efecto de los \emph{outliers} (valores atípicos).
Un valor extremo en normalización puede hacer que la visualización sea muy
pobre, juntando la mayoría de los puntos a una zona. Con la estandarización este
efecto es mucho menor, pues se tiene en cuenta la media de los valores.

\clearpage
\section{Desarrollo de los algoritmos}

Junto con la aplicación web, este desarrollo es el más importante realizado. Ha
supuesto un verdadero reto documentarse con los artículos científicos de estos
algoritmos. Al fin y al cabo, no se tienen absolutamente todos los conocimientos
del aprendizaje y en muchas ocasiones se <<perdía>> más tiempo leyendo y
entendiendo que programando. Sí que es verdad que los algoritmos que se han
desarrollado tienen ideas bastante lógicas y directas.

Para desarrollar un algoritmo lo primero que se realizaba era leer y entender la
teoría que se presentaba en los artículos, consultando conceptos que no se
entendían en internet. En segundo lugar, en todos ellos siempre aparecía un
pseudocódigo que describía formalmente todo el proceso del algoritmo y que luego
se pasaba a la implementación. Exceptuando Self-Training, no era posible
entender su funcionamiento sin entender la teoría y la dificultad se encontraba
en que no se había trabajado con este tipo de notaciones particulares y era muy
fácil perderse.

Entrando más en cómo se desarrollaban, la idea siempre era crear un objeto con
ese algoritmo. De hecho, al principio del proyecto (con Self-Training) se
realizaba como una función única. Sin embargo, la idea de la orientación a
objetos permitía un manejo mucho más fino para crear funciones separadas
(entrenamiento, estadísticas, predicciones...) pero del mismo contexto y
facilitaban su uso en la parte de la aplicación Web (Flask).

Por lo general, se creaban unas versiones básicas en las que simplemente se
quería tener una implementación funcional. En esas primeras fases no se
consideraban <<refactorizaciones>> en cuanto a estructuras de datos más óptimas
o exceso de complejidad, sino que era una toma de contacto para ajustar el
pensamiento a cada uno de ellos en particular. Para probar estos algoritmos de
forma rudimentaria, se utilizaban los conjuntos de datos de <<juguete>> de
\texttt{Scikit-Learn}.

Cuando se pensaba que la implementación correspondía (en principio) a los
artículos, se comenzaban esas tareas de depuración. Por ejemplo, tanto en
Democratic Co-Learning como en Tri-Training se detectó que los tiempos de
entrenamiento eran demasiado grandes. En el primer caso, resultó ser porque la
condición de parada estaba mal planteada y ejecutaba muchas más iteraciones de
las necesarias. En el segundo caso, resultó ser precisamente el uso de
estructuras de datos más lentas (se subsanó utilizando \texttt{numpy}).

\subsection{Validación}

Aparte de intentar depurar el código generado, no era suficiente con la
intuición de que estos algoritmos eran correctos. Para comprobarlo, se ha
comparado con la biblioteca \texttt{sslearn}
~\cite{jose_luis_garrido_labrador_2023_7781117}. Esta biblioteca cubre todos los
algoritmos desarrollados en este proyecto y que además está mantenida
periódicamente.

Antes de presentar los resultados de esta validación, se definen las
estadísticas que se han utilizado para comprobar que la implementación propia es
comparable con \texttt{sslearn} y concluir que es correcta.

\begin{itemize}
    \item \textit{\textbf{Accuracy}}: Esta métrica será el núcleo principal de
    la comparación. Representa la proporción de aciertos entre todos los datos.

    \item \textit{\textbf{F1 score}}: Esta métrica se calcula como la media
    armónica de la precision\footnote{Precision: proporción de clasificados
    positivos (verdaderos positivos + falsos positivos) que son clasificados
    positivos (verdaderos positivos)} y \texttt{recall} (es igual que la
    sensibilidad).

    \begin{center}
        $ F_1 = 2 \cdot \frac{precision \cdot recall}{precision + recall} $
    \end{center}

    \item \textit{\textbf{Geometric mean (Gmean)}}: En este contexto, la media
    geométrica trata maximizar cada una de las medidas de \emph{accuracy} de
    cada clase~\cite{imbalanced_learn}.

    Para realizar su cálculo, se tiene en cuenta si se trata de clasificación
    binaria o multiclase:
    \begin{itemize}
        \item Binaria: la media geométrica se calcula como la raíz cuadrada del
        producto de la sensibilidad\footnote{Sensibilidad: proporción de
        positivos correctamente identificados (clasificados como positivos
        respecto a todos los positivos)~\cite{eswiki:145396343}} y la
        especificidad\footnote{Especificidad: proporción de negativos
        correctamente identificados (clasificados como negativos respecto a
        todos los negativos)~\cite{eswiki:145396343}}.

        \begin{center}
            $ Gmean = \sqrt{\textit{sensibilidad} \cdot \textit{especificidad}} $
        \end{center}
        

        \item Multiclase: la media geométrica se calcula como la raíz $n$-ésima
        del producto de las sensibilidades de cada clase.

        \begin{center}
            $Gmean = \sqrt[\leftroot{5} \uproot{5} n]{\textit{sensibilidad}_1 \cdot
            \textit{sensibilidad}_2 ~...~..~\cdot \textit{sensibilidad}_n }$
        \end{center}
    \end{itemize}

    El mejor valor es 1 y el peor 0.
\end{itemize}

La comparación consistirá en una validación cruzada con 10 \textit{folds} con
tres conjuntos de datos diferentes. Por cada iteración de la validación cruzada
se tendrá el 90\% de datos para entrenar y 10\% de test.

Sobre ese 90\% de entrenamiento, solo el 20\% va a estar etiquetados, el resto
serán no etiquetados. En conjunto, esos serán los datos para entrenar el
algoritmo.

Por cada uno de los conjuntos de datos se presenta un gráfico comparando cada
algoritmo propio con el de \texttt{sslearn} mediante gráficos de caja. En el eje
$X$ se presentan los algoritmos que se están comparando y en el eje $Y$ cada una de
las estadísticas antes comentadas.

Contenido de cada gráfico de cajas:
\vspace{-0.4cm}
\begin{itemize}
    \item \textbf{Línea continua azul}: representa la media.
    \item \textbf{Línea discontinua negra}: representa la mediana (segundo
    cuartil).
    \item \textbf{Borde superior de la caja coloreada}: representa el tercer cuartil.
    \item \textbf{Borde inferior de la caja coloreada}: representa el primer cuartil.
    \item \textbf{Límite del bigote superior}: representa el máximo de los valores
    encontrados.
    \item \textbf{Límite del bigote inferior}: representa el mínimo de los valores
    encontrados.
\end{itemize}

En la tabla~\ref{tabla:clasificadores} se pueden ver los clasificadores base
seleccionados para cada algoritmo. La selección es completamente arbitraria (con
base en los clasificadores que se han usado personalmente en el proyecto y fuera
de él).

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|l|l|l}
    \textbf{Self-Training}         & \textbf{Co-Training}                             & \textbf{Democratic Co-Learning}                                              & \textbf{Tri-Training}                      \\ \hline
                                   & Árbol de decisión & Vectores de Soporte C (SVC)  & Árbol de decisión \\
    Naive Bayes Gaussiano          & Naive Bayes Gaussiano & Naive Bayes Gaussiano & kNN \\
                                   &  & Árbol de decisión & Naive Bayes Gaussiano \\
    \end{tabular}%
    }
\caption{Clasificadores base}
\label{tabla:clasificadores}
\end{table}


\imagen{memoria/Breast}{Comparación de métodos con el conjunto de datos \textit{Breast Cancer}.}{1}

\imagen{memoria/Iris}{Comparación  de métodos con el conjunto de datos \textit{Iris}.}{1}

\imagen{memoria/Wine}{Comparación  de métodos con el conjunto de datos \textit{Wine}.}{1}


Por cada una de las figuras anteriores, lo interesante para comprobar si las
implementaciones pueden considerarse como correctas es fijarse en
\texttt{sslearn\footnote{Biblioteca de algoritmos semi-supervisados creada por
José Luis Garrido-Labrador~\cite{jose_luis_garrido_labrador_2023_7781117}}}.

Como se puede ver, en todos los casos los valores de la implementación propia
coinciden en gran medida con las de \texttt{sslearn}. Es de destacar que en los
casos de Self-Training, Democratic Co-Learning y Tri-Training se comportan de
manera muy similar. En el caso de Co-Training en todas hay más variabilidad.
Esto se debe a las diferencias de implementación.

Aunque haya pequeñas variaciones, los valores obtenidos son muy parecidos. Se
concluye por tanto, que los algoritmos implementados se consideran correctos.


\section{Desarrollo web}

Pese a que los algoritmos han sido una buena parte de todo el proyecto, el
desarrollo de la aplicación web es el que ha ocupado la mayor parte del tiempo
(de forma aproximada, se calcula que un 70\%). El por qué esto ha sido así
reside en dos cuestiones. 

Por un lado, el tamaño de la aplicación, que pese a no ser una aplicación
extremadamente grande, todo se ha realizado desde cero. No se adquirió ninguna
plantilla o estructura existente y la única ayuda utilizada fue
\texttt{Bootstrap} (que sí agiliza el estilado de la web). Y en segundo lugar,
por el desconocimiento del desarrollo web. Durante los estudios no se han dado
asignaturas de esta temática sea el desarrollo web. No ha sido hasta este mismo
curso cuando se nos ha introducido el mundo web (Diseño y Mantenimiento del
Software).

Durante todo el desarrollo, y hasta el final, se tomaron decisiones
constantemente. El \emph{framework} Flask fue el que se manipuló en esa
asignatura, era interesante seguir en esa línea y desarrollar el proyecto con
él. 

De hecho, al comenzar el proyecto, se tomó las estructura básica del proyecto
final de la asignatura dejando solo lo básico para arrancar una aplicación Web.

\subsection{JavaScript}
La toma de contacto con JavaScript fue compleja, no se había trabajado con este
lenguaje y solo se tenían conceptos básicos de HTML y CSS (junto con
\emph{backend}, que al ser Python, ya se tenía cierta soltura). Para intentar
avanzar lo máximo posible, se optó por ver tutoriales y
cursos\footnote{\url{https://www.w3schools.com/}} al mismo tiempo que ya se
iniciaban las visualizaciones de los algoritmos (completamente rudimentarias).
En este sentido, se iba a utilizar la librería \texttt{D3.js} que, comparándola
con otras, era algo más difícil de manejar con soltura.

Esta fue la línea general de desarrollo en JavaScript, en el momento que se
presentaba un problema o el uso de una nueva librería, ir a fondo con ello
directamente sobre la Web, hasta conseguir avances. Todo ello guiado por foros o
páginas como \textit{Stack Overflow}. Además, se optó por desarrollar la Web en
JavaScript <<Vanilla>>, sin librerías como jQuery. Se pensó que aprender las
bases de JavaScript era más provechoso.

Es destacable la cantidad de veces que hubo que refactorizar el código. Durante
el manejo y aprendizaje, aparecían mejores soluciones a lo que ya se tenía. Por
ejemplo, el uso de \emph{imports} permitía centralizar y reutilizar código (que
al principio se desconocía).

Aparte de todo esto, exceptuando las visualizaciones (que son bastante
particulares) y considerando todos los problemas que aparecían, el resto del
código de JavaScript no era extremadamente complejo y la mayoría de las veces
existían soluciones parciales que simplemente podían adaptarse. 

El avance, aunque algunas veces lento, era constante.

\subsection{Rediseño completo}

Uno de los puntos más destacables del desarrollo fue durante los sprints 13 y
14. La aplicación tuvo un rediseño completo (tantoen  estructura como
apariencia) partiendo desde el <<backend>> hasta el <<frontend>>.

Tal y como estaba la aplicación estructurada, todas las rutas estaban en un
único fichero, que a su vez era el mismo que instanciaba la aplicación. Esto,
para demos o aplicaciones muy pequeñas, es una solución rápida y buena. Sin
embargo, lo que se vio de otros proyectos es que cuando el tamaño es grande, es
necesario compartimentar rutas, modelos de bases de datos u otros elementos que
crecen en número. Tener absolutamente todo en un único fichero es engorroso.

Es por eso que se utilizó la idea de \textit{Application Factory} y
\textit{Blueprints}. Por un lado, \textit{Application Factory} ha permitido
hacer un único punto de creación de la aplicación donde se configura y se
desvincula del resto del código.

Los \textit{Blueprints} son como pequeñas aplicaciones independientes de Flask
(que luego son unificadas en una, en esa \textit{Application Factory}
comentada). En cada \textit{Blueprint} se han definido las rutas que intervienen
en cada contexto. Por ejemplo, para la configuración de los algoritmos, se han
definido sus rutas exclusivas, como si fuera una aplicación por sí sola. Esto
permite aplicaciones \textbf{mantenibles y escalables}.

Tal y como está al final, con la cantidad de rutas y utilidades codificadas,
hubiera sido imposible mantener un único fichero para toda la aplicación.

\subsection{Babel}

Un aspecto a destacar de la aplicación Web del proyecto es la
internacionalización. En principio, dado que no era necesario abarcar un abanico
grande de idiomas, la aplicación está pensada para Inglés y Español, con la
ventaja de que \texttt{Babel} hace muy fácil el manejo de las traducciones y es
perfectamente escalable. Desde las primeras fases de desarrollo se incluyó esta
forma de mantener unas traducciones de forma automática, y junto con el
mantenimiento de las traducciones actualizadas cada poco tiempo, la tarea no ha
resultado difícil, aunque sí laboriosa.

\section{Despliegue básico}

Añadido a los objetivos del proyecto, se quiso aprender cómo se despliega una
aplicación web cuando se dispone un servidor <<propio>> (comprado o alquilado).
En esta sección se va a comentar a grandes rasgos cómo se ha desplegado.

El servidor donde está establecida la aplicación se trata de un servicio
\textit{Cloud} exactamente igual a lo que sería tener un ordenador, pero en este
caso no tiene acceso físico, sino remoto.

La aplicación en sí (lanzada mediante la ejecución normal de Flask) no puede ser
accedida desde el exterior pues Flask se ejecuta en local (al igual que se ha
hecho en el desarrollo accediendo a \url{127.0.0.1}).

Para permitir el acceso desde el exterior, se ha establecido un \emph{proxy
inverso}. Este tipo de \textit{proxy} tiene dos ideas importantes, en primer
lugar, permite no conocer exactamente qué es lo que el servidor tiene
funcionando por dentro, y por otro, permite redirigir las peticiones que recibe
el servidor a las aplicaciones o puertos que se deseen (actúa como
\emph{servidor web}).

Esto se ha realizado con \texttt{Nginx}, que es fácil de configurar y es muy
usado. Como las peticiones que se realizan para acceder a una página web se
hacen sobre el puerto 80, en la configuración de \texttt{Nginx} se ha incluido
una entrada que redirecciona las peticiones de ese puerto (dirección IP del
servidor) a \url{localhost:5000} (que es donde se ha considerado ejecutar la
aplicación Flask).

Otro aspecto a destacar es que se ha adquirido el dominio \texttt{dmacha.dev}, y
se tiene también un \emph{certificado SSL} proporcionado por \textit{Let's
Encrypt}. Con el certificado <<instalado>> e indicándole a \texttt{Nginx} el
dominio (utilizar el dominio como si fuera IP del servidor), la aplicación es
completamente accesible desde internet.

Con todo lo anterior, existe la posibilidad de ejecutar el servidor propio de
Flask (que suele usarse en un entorno de desarrollo), y un servidor WSGI (Web
Server Gateway Interface\footnote{Indicará cómo comunicar \texttt{Nginx} (en
este caso) con la aplicación Flask.}) en entornos de producción. Para este
servidor WSGI se ha considerado \texttt{gunicorn} (\textit{green unicorn}), una
utilidad de Python que permite levantar aplicaciones como Flask. 

En una única ocasión con \texttt{gunicorn}, el tiempo de respuesta alcanzaba los
2 minutos o incluso no respondía (los procesos que manejaban las peticiones se
reiniciaban constantemente debido a la falta de recursos). De este modo, y como
no se obtuvieron más datos de lo ocurrido, se considera la posibilidad de
ejecutar directamente el servidor Flask para mantener el servicio en caso de
fallos, pero siempre con preferencia por WSGI.