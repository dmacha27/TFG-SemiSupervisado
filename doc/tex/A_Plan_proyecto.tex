\apendice{Plan de Proyecto Software}

\section{Introducción}

En el presente apartado de los anexos se analizará la gestión del proyecto
software desarrollado. Este proyecto será organizado mediante la metodología
Scrum en la que el trabajo estará dividido en Sprints. Por cada Sprint se
realiza una reunión para la revisión del avance y los objetivos para el
siguiente.  Con esta metodología se mantendrá en todo momento lo que se conoce
como \textit{Product Backlog} que es una lista de las tareas a realizar. Esta
lista será actualizada, en principio, en cada reunión para así mantener un
desarrollo constante. Las reuniones en un principio se realizan cada dos
semanas, intensificando a cada semana en el momento del inicio del periodo
temporal del segundo cuatrimestre.

El objetivo de este plan es servir como herramienta para registrar el avance del
proyecto y también para poder cumplir con el objetivo final del desarrollo.


\section{Planificación temporal}
La planificación temporal se comenzó mediante Sprints de dos semanas. En la
presente sección se comentará el desarrollo realiza en cada uno de ellos.

\subsection{Sprint 0}

Desde el punto de vista temporal corresponde desde el inicio del curso del
primer cuatrimestre académico (septiembre) hasta el Sprint 1. El día 15 de
septiembre se tuvo la primera reunión con los tutores sobre el trabajo presente
donde se establecieron las líneas generales y temática sobre el mismo.

Se creó el repositorio del TFG en Github:
\url{https://github.com/dma1004/TFG-SemiSupervisado} y se añadió la plantilla de
la documentación.

\subsection{Sprint 1}

Corresponde con el periodo temporal del 5 al 19 de octubre de 2022. 

El mismo día 5 tuvo lugar una reunión de seguimiento del trabajo. Durante el sprint se
realizaron unos arreglos de la plantilla y una lectura de conceptos teóricos
para posteriormente añadirlos a la documentación. Concretamente se crearon las
tareas "<Añadir conceptos teóricos aprendizaje"> y "<Trabajos relacionados"> a
día 9 de octubre.

\subsection{Sprint 2}

Corresponde con el periodo temporal del 19 de octubre al 2 de noviembre de 2022. 

Durante el sprint se implementó un prototipo del algoritmo Self-Training en el
que posteriormente se hicieron unas correcciones en el código. También se
comenzó con la redacción de conceptos teóricos (tarea "<In progess">), concretamente, sobre el
aprendizaje automático.


\subsection{Sprint 3}

Corresponde con el periodo temporal del 16 al 30 de noviembre de 2022.

Durante el sprint se aumentaron los conceptos teóricos sobre el aprendizaje
supervisado, no supervisado y semi-supervisado. Se refactorizó el prototipo para
su documentación (PEP), evitar datos duplicados y modularizando el código.

La memoria fue parcialmente modificada basándose en las correcciones propuestas
de los tutores.

\subsection{Sprint 4}

Corresponde con el periodo temporal del 25 de enero al 1 de febrero de 2023. En este
momento las duraciones de los Sprints cambiaron a una semana, iniciando así el
periodo temporal real del desarrollo del proyecto (segundo cuatrimestre)

Durante el sprint se retomaron las tareas y el desarrollo general del proyecto.
Se mejoró el algoritmo de \texttt{SelfTraining} que estaba como prototipo y se avanzó en
la tarea de primera aproximación en la aplicación. mediante Flask. Sobre esto último, se creó
una visualización del proceso de entrenamiento muy básica por cada iteración.

Se creó un prototipo del algoritmo \texttt{CoTraining} sin cumplir con todas sus
condiciones que posteriormente se completaron a falta de revisión. 

Sobre estos dos algoritmos se propuso la versión 1.0. 

Continuando con la Web, se realizó la interfaz general funcional. Incluye:
\begin{itemize}
    \item Página de Inicio donde seleccionar el algoritmo.
    \item Página de subida de archivos en formatos ARFF y CSV de los conjuntos de datos
    \item Páginas correspondientes para SelfTraining y CoTraining: Cada una
    tiene sus parámetros específicos con la posibilidad de seleccionar si
    utilizar PCA (Principal Component Analysis) o dos componentes que elija el usuario.
    \item Página de visualización del algoritmo (su entrenamiento): Se tiene la
    vista principal que será común a todos los algoritmos (con algunas
    variaciones en caso necesario) con la posibilidad de avanzar en la
    visualización (con controles) y barra de progreso. Desde el punto de vista
    del gráfico los colores están automatizados dependiendo del número de
    clases, leyenda y etiqueta de ejes. 
\end{itemize}
En el servidor (Flask) a nivel de programación se añadieron los "<endpoints">
correspondientes (subida, configuración, visualización...) y un control de
acceso a las páginas muy básico (por ejemplo, si no se configuró el algoritmo,
no se puede visualizar y le redirecciona a la configuración con un mensaje de error)

\subsection{Sprint 5}
Corresponde con el periodo temporal del 1 al 8 de febrero de 2023.

En la reunión del 1 de febrero se revisó lo realizado en el anterior y se fijaron
una serie de mejoras/modificaciones y nuevas tareas:
\begin{enumerate}
    \item Modificación de los algoritmos para trabajar con la convención de
    "<-1s"> en el conjunto de datos para los datos no etiquetados. Así el
    usuario podrá subir un archivo ya \textit{Semi-Supervisado}.
    \item Permitir al usuario seleccionar los porcentajes de no etiquetados y de
    test (para las futuras estadísticas).
    \item Sobre la página general de la visualización de los algoritmos: volver
    a la configuración, el "<feedback"> de la iteración actual y el nombre del
    conjunto de datos utilizado.
    \item Del gráfico de la visualización: Diferenciar en el algoritmo
    CoTraining cuál de los dos clasificadores han etiquetado cada punto y los
    puntos "<etiquetados"> en la iteración 0 deben mostrarse de forma diferente.
    \item Avanzar con los trabajos relacionados.
    \item Avanzar con la documentación teórica y anexos.
\end{enumerate}

El punto 1 ha llevado unas 12 horas de compresión y desarrollo. Esto es debido a
que los dos algoritmos implementados hasta ahora debían ser modificados para
trabajar con la nueva convención. Además, el problema principal fue (aunque no
implementado en este Sprint) dejar preparado una forma de carga del conjunto de
datos que permita tratar datos no etiquetados ("<?"> por ejemplo en el caso de
ARFF) pues además de los algoritmos (su correcto funcionamiento) se han probado
con ficheros. También conllevó la creación de un codificador de etiquetas propio
para ignorar los no etiquetados en clases categóricas (y no realizar la
conversión en esos casos)

El punto 2 volvió a causar bastantes problemas tanto en la ejecución de los
algoritmos como en la Web. Hasta el momento, el usuario no seleccionaba los
porcentajes de las divisiones. Al incluir esto, los algoritmos ya no se encargan
de esta tarea y había que modificar tanto los algoritmos como aquellas rutas de
la Web que debían encargarse de esto. Aproximadamente 4 horas.

El punto 3 no resultó demasiado difícil más allá de seguir habituándose a
JavaScript/HTML. Unas 3 horas.

El punto 4 requirió unas 10 horas, en un principio se perdió mucho tiempo
intentando solucionarlo de una forma que resultó inútil, pero finalmente ahora en
el algoritmo se diferencian los datos clasificados por cada uno.

Los trabajos relacionados (no terminados) se realizaron en varios días con un
tiempo aproximado de 6 horas.

\subsection{Sprint 6}
Corresponde con el periodo temporal del 8 al 15 de febrero de 2023.

En la reunión del 8 de febrero se revisó lo realizado en el anterior y se
comentaron algunas tareas a realizar:
\begin{enumerate}
    \item En la línea del anterior, los algoritmos deben poder ejecutarse
    directamente con conjuntos de datos semi-supervisados.
    \item Permitir al usuario introducir ese tipo de conjuntos de datos.
    \item Realizar alguna visualización de estadísticas.
    \item Valor por defecto en las configuraciones.
    \item Sobre el gráfico: mejorar la diferenciación de los puntos, información
    útil en los "<tooltips"> y colocación leyenda.
    \item Avanzar con los trabajos relacionados.
    \item Avanzar con la memoria y anexos.
\end{enumerate}

Los puntos 1 y 2 estaban muy avanzados gracias al trabajo adicional del sprint
anterior, ya que ya estaba prácticamente implementada la forma en la que
detectar datos no etiquetados de forma automática. Unas 5 horas para terminar de
implementar, corregir errores sobre la marcha y realizar alguna prueba
confeccionando ficheros semi-supervisados.

Al realizar las pruebas anteriores se encontró un error en la visualización
provocando que los datos que, por la iteración máxima, no se habían clasificado
ni siquiera eran retornadas a la Web. Entre descubrir cómo hacerlo y sus
modificaciones se tardó unas 3 horas.

El punto 3 fue el más complicado, pese a que era una idea sencilla, se optó por
visualizar la gráfica de la evolución de la precisión. Cada punto del gráfico
está unido por una serie de líneas. Este tipo de gráficos (según la
documentación) se suelen hacer mediante "<paths"> o caminos, que son una única
línea, pero como en este caso era necesario no visualizar todo, sino por cada
iteración, no se encontró una solución rápida. Unas 6 horas para probar muchas
posibilidades hasta encontrar la que funcionó, acoplarla a los controles del
paso de iteración e incluir alguna animación.

Adicionalmente se retocó por completo toda la Web mediante los estilos de
\texttt{Bootstrap} para establecer ya una base vistosa y bonita. Unas 5 horas
(la mayor parte del tiempo para probar y adquirir algo de soltura con estos
estilos).

\subsection{Sprint 7}
Corresponde con el periodo temporal del 15 al 22 de febrero de 2023.

Puntos a desarrollar:
\begin{enumerate}
    \item Implementación Democratic Co-Learning.
    \item Profiling (tiempos de ejecución).
    \item Estadísticas en la aplicación.
    \item Test de las implementaciones.
    \item Avanzar con la memoria y anexos.
\end{enumerate}

La implementación del algoritmo Democratic Co-Learning supuso unas 14 horas
divididas en varios días. Al principio se dedicó un tiempo para leer el artículo
en el que se presentaba su implementación en forma de pseudocódigo junto con sus
explicaciones teóricas. La realidad es que en primera instancia parecía algo
fácil de realizar y entender, pero una vez comenzada la implementación se
encontraban muchas alternativas a la hora de resolverlo. Además, pese a que en
el artículo estaba bien explicado, el formato de pseudocódigo (en el archivo
encontrado) las indentaciones eran incorrectas y se perdió mucho tiempo
comprobando si era una interpretación errónea o si realmente era un fallo.

Se realizaron algunas pruebas de rendimiento para comprobar si los algoritmos
tardaban demasiado con conjuntos de datos muy grandes (5\,000 instancias). Se
observó que, dada la configuración que se tenía, tardaba alrededor de 40-50
segundos en terminar la ejecución. Es por esto que para este Sprint se añadió la
tarea de hacer un pequeño estudio dedicado a medir los tiempos de ejecución para
ver qué se podía optimizar. Este proceso fue de unas 2 horas y el resultado fue
que el código implementado no afectaba mucho, eran los propios algoritmos de
entrenamiento de los clasificadores de Scikit-Learn los que tardaban tanto. Por
ejemplo, para un estimador gaussiano el tiempo se reducía drásticamente.

Para el caso de las estadísticas, se modificaron un poco las plantillas y la
generación de sus gráficas para incluir más y revisarlas en la reunión. Unas 2
horas.

Los tests son una parte importante para validar que el comportamiento que se
espera de la implementación sea el correcto. Se realizaron unos casos de pruebas
sobre las utilidades que se usan a lo largo de todo el proyecto con la intención
de encontrar errores (todo esto sin ver cuál es el resultado y replicarlo en los
casos, sino realizar los casos basándose en lo que se espera de esas
utilidades). Se tardó unas 4 horas en realizar todos los tests.

\subsection{Sprint 8}
Corresponde con el periodo temporal del 22 de febrero al 1 de marzo de 2023.
Además, aprovechando la herramienta Zenhub, se modificó la duración de los
Sprints también en ella para poder extraer los gráficos del trabajo realizado.

Puntos a desarrollar:
\begin{enumerate}
    \item Intervalo de confianza en Democratic Co-Learning.
    \item Control reetiquetado en Democratic Co-Learning.
    \item Correcciones sobre memoria y anexos.
    \item Gráfico de estadísticas unificado.
    \item Internacionalización Web.
    \item Visualización principal de Democratic Co-Learning en la aplicación.
\end{enumerate}

El primer punto fue muy sencillo, se proporcionó la implementación de los
intervalos de confianza tanto de Álvar Arnaiz González como de César Ignacio
García Osorio (tutores) y finalmente se implementó esta segunda.

El control del reetiquetado fue mal estimado (en puntos de historia), al
principio parecía una idea sencilla, pero por un error de pensamiento, la
implementación que se realizó en un principio no funcionaba. Como cada
clasificador tiene su propio conjunto de entrenamiento, se estaba tomando que el
índice de la instancia sumado a la longitud de su conjunto de entrenamiento era
la posición en la que actualizar la etiqueta, obviamente esto no funciona pues
esa suma puede superar la longitud del propio conjunto. Había que almacenar la
posición concreta sin realizar esos cálculos y se optó por un diccionario de
<<ids>> para cada clasificador en el que el valor es la posición en las
etiquetas del conjunto del clasificador.

Las correcciones de la documentación se incorporaron según las indicaciones de
Álvar Arnaiz.

El gráfico de estadísticas fue unificado, permitiendo seleccionar al usuario
mediante unos <<checkboxes>>. Aproximadamente unas 4 horas para generar el
formulario correspondiente que controle la aparición de cada línea y controlar
los eventos de las iteraciones (por ejemplo, añadir siguiente punto solo si está
activado su check).

En cuanto a la Internacionalización, al principio resultó sencillo, ya que
gracias a Babel (Flask-Babel) detecta automáticamente las cadenas de texto
dentro de <<gettext>>, sin embargo, al indicar las traducciones en JavaScript,
el intérprete tomaba <<gettext>> como una función que al no estar definida,
lanzaba error. Al final se generó una función en la plantilla principal de tal
forma que actúe como <<gettext>>, llamada desde los distintos scripts.

La visualización de Democratic Co-Learning no dio tiempo a implementarse en este Sprint.

Además, a partir de este Sprint se incorporan todas las tareas en Zenhub para la
generación de los gráficos Burndown (ver Imagen~\ref{fig:sprints/sprint8}).

\imagen{sprints/sprint8}{Burndown chart del sprint 8.}


\subsection{Sprint 9}
Corresponde con el periodo temporal del 1 de febrero al 8 de marzo de 2023.

Puntos a desarrollar:
\begin{enumerate}
    \item Visualización principal de Democratic Co-Learning en la aplicación.
    \item Formulario de los parámetros de los clasificadores.
    \item Estadísticas generales en Democratic Co-Learning.
    \item Estadísticas específicas en Democratic Co-Learning
    \item Condición de parada reetiquetado
\end{enumerate}

La visualización principal fue relativamente sencilla pues en realidad es muy
similar a Co-Training. Hubo algún ajuste adicional para generar la información
de la \textit{tooltip} al pasar por encima de un punto. Como en cada posición podía haber
varios clasificadores había que mostrar esa información.

Para el formulario de los parámetros se consideró el uso de Flask-WTF que al
final se descartó. Se tenía como punto de partida utilizar un JSON del que leer
los parámetros, así que lo primero fue pensar una forma sencilla de
codificarlos, con la información necesaria de las entradas
(<<type>>,<<step>>...). La ventaja de JSON es que aparte de que la lectura es
muy sencilla, son diccionarios, muy fáciles de utilizar (tanto desde Python como
desde las propias plantillas/JavaScript). Para generar el formulario se pensó en
hacerlo de la forma más automática posible para así solo tener que cambiar el
JSON, esto se hizo con un método de JavaScript que para cada clasificador genera
el formulario correspondiente con los parámetros del JSON. El tiempo total fue
unas 6 horas, pues también se perdió tiempo debido a que no era posible
seleccionar elementos del DOM (pues no estaba cargado) y los elementos debían
seleccionarse mediante CSS (algo que no se sabía por desconocimiento de
JavaScript).

Se añadieron las estadísticas generales de Democratic Co-Learning de forma muy
sencilla pues era exactamente igual que Co-Training (y Self-Training) esto se
realizó añadiendo el \textit{DataFrame} en el propio algoritmo con las
estadísticas deseadas (como el resto de los algoritmos).

Había mucho código muy parecido o exactamente igual en las plantillas de los
algoritmos, así que aprovechando esta tarea también se automatizaron por
completo las estadísticas. Se pensó de tal forma que solo con las columnas de
los \textit{DataFrames} que retornan los algoritmos, la Web ya se encargue de
generar el resto. A la vez que esto (e iniciando la tarea de las estadísticas
individuales), otro punto importante que se tuvo en cuenta eran las futuras
estadísticas individuales para Democratic Co-Learning, todas las funciones
fueron transformadas para trabajar con elementos del DOM específicos, de esta
forma al indicarle por ejemplo un <<DIV>>, se generen las estadísticas sobre ese
elemento. En total unas 8 horas. 

Para estas estadísticas individuales, aparte de las funciones generalizadas se
creó una nueva que sobre un elemento (un <<DIV>>) se generase un selector con el
nombre de los clasificadores que intervienen en el algoritmo junto con los
contenedores dentro de él para las estadísticas de cada clasificador.
Finalmente, se utilizan las funciones de la tarea anterior para añadir a esos
contenedores nuevos los gráficos individuales. Unas 4 horas.

Sobre el último punto, se comentó que una etiqueta que es reetiquetada al mismo
valor no debe <<contar>> como mejora (o cambio en el algoritmo). Si no se
realiza así, el tiempo de ejecución aumenta demasiado.


El gráfico Burndown se visualiza en la imagen~\ref{fig:sprints/sprint9}.
\imagen{sprints/sprint9}{Burndown chart del sprint 9.}

\subsection{Sprint 10}
Corresponde con el periodo temporal del 8 de febrero al 15 de marzo de 2023.

Puntos a desarrollar:
\begin{enumerate}
    \item Comparación sslearn
    \item Refactorización de plantillas.
    \item Refactorizacion Javascript.
    \item Refactorización Flask (app).
    \item Añadir zoom a los gráficos.
    \item Conceptos teóricos.
\end{enumerate}

En este Sprint no se contaba con demasiado tiempo así que aunque sí se realizó
alguna tarea para añadir funciones, la idea era dedicarlo a mejorar el código y
mantenimiento.

En la reunión del final del Sprint anterior se comentó el cómo se estaba
validando los algoritmos y hasta ese momento solo se habían probado las
utilidades. Se sugirió compararlo contra \textit{sslearn}, una biblioteca de José Luis
Garrido-Labrador. Como primera aproximación se realizó una validación cruzada
completamente manual en la que se ejecutaba al mismo tiempo las dos
implementaciones de los distintos algoritmos (de momento sin extraer
conclusiones). Se tardó unas 3 horas.

El segundo y tercer punto se iniciaron por separado, pero llegó un momento en el
que las funciones que se había creado en JavaScript, si se modificaban un poco,
podrían simplificarse a su vez las plantillas. Todos los métodos los gráficos
estaban separados por cada uno de los algoritmos, esto lo hacía muy engorroso
porque incluso algún método tenía el mismo nombre. Se modificaron las funciones
de tal forma que fuesen específicas para cada algoritmo para así juntarlas en un
único Script. Por parte de las plantillas, como había partes repetidas se
crearon macros y se identificó una parte común a todos los algoritmos en su
configuración, esto se añadió a la base de las configuraciones. Todo esto llevó
unas 4 horas.

En \texttt{Flask} se tenían varios problemas. El primero era que las
visualizaciones de cada algoritmo tenían un \textit{endpoint} particular, pero
esto no era necesario si se hacía un método para cada la obtención de los
parámetros de cada algoritmo. Cuando se crearon estos métodos se generó código
muy parecido porque todos ellos tenían dos partes: una en la que se obtenían los
parámetros que no eran de los clasificadores base y otra en la que se
incorporaban esos parámetros de los clasificadores. La primera parte es
particular para cada algoritmo, pero la segunda es común a todos. Se creó otro
método para ese paso. Por último, por cada algoritmo se tiene un
\textit{endpoint} para la ejecución y obtención de la información de
entrenamiento, pero había una parte que todos hacían prácticamente igual. Se
creó un método que engloba: carga de datos, separación de los datos para
entrenamiento, entrenamiento y obtención de los algoritmos y la aplicación de
PCA (o no).

La visualización principal de los algoritmos tenía el problema de que cuando los
puntos estaban demasiado cerca, no se llegan a apreciar individualmente. Esto se
solucionaría aplicando \textit{zoom} al gráfico. Pese a que en cuanto a código
no fuese un desarrollo largo, fue una tarea compleja. Se probaron unas tres
implementaciones parecidas a ejemplos encontrados en la documentación, pero en
todas ellas se perdía la ayuda contextual del \textit{tooltip}. Al final se optó
por volver a empezar de cero con el conocimiento adquirido y junto con un último
ejemplo \footnote{Ejemplo D3:
\url{https://observablehq.com/@d3/zoom-with-tooltip}} y ciertas modificaciones
se consiguió. Posteriormente se añadió el reinicio del \textit{zoom} para volver
a la posición original. El proceso duró unas 5 horas pues cada intento parecía
ser definitivo, pero al final siempre había ciertos límites.

Al final del Sprint se añadieron los conceptos teóricos de Democratic
Co-Learning (conceptos y pseudocódigos).

Aparte de estas tareas se realizaron pequeñas modificaciones: se completó el
estilo adaptable (<<responsive>>), se arreglaron pequeños bugs de
visualizaciones y ayuda contextual, actualización de traducciones y se añadió un
fichero de prueba que el usuario puede descargar si solo quiere probar la
aplicación.

El gráfico Burndown se visualiza en la imagen~\ref{fig:sprints/sprint10}.
\imagen{sprints/sprint10}{Burndown chart del sprint 10.}

\subsection{Sprint 11}
Corresponde con el periodo temporal del 15 al 22 de marzo de 2023.

Puntos a desarrollar:
\begin{enumerate}
    \item Arreglos en Democratic Co-Learning.
    \item Controlar los límites en los parámetros de los clasificadores.
    \item Modificar las estadísticas generales.
    \item Invertir controles en las estadísticas específicas.
    \item Anexos: Manual del programador.
    \item Modificar validación cruzada.
    \item Comparación exhaustiva contra sslearn.
    \item Añadir validación de los algoritmos a la memoria.
\end{enumerate}

En Democratic Co-Learning se estaba obviando el caso en el que si una instancia
ya estaba etiquetada y esta es reetiquetada, pero además cambiando de etiqueta,
no se consideraba como cambio en el algoritmo. Se ha añadido esta casuística.
Además, Álvar sugirió en el pseudocódigo de la memoria hacer el método de
combinación de hipótesis (predicción) para una sola instancia. Esto se ha
realizado así en la propia implementación (y así el método \texttt{predict} se
encarga de iterar sobre un conjunto de instancias).

Al JSON que codifica los parámetros de los clasificadores base se añadieron los
controles de mínimo y máximo. Esto es porque hay algunos de sus parámetros que
requieren rangos específicos. Ahora la web (JavaScript) genera el formulario de
configuración teniendo en cuenta estos límites.

En la reunión del Sprint anterior se sugirió modificar la visualización de
estadísticas. Para el gráfico general (de estadísticas) no tenía sentido poder
ocultar o no cada una de las estadísticas así que ahora siempre se muestran
todas ellas. Particularmente para Democratic Co-Learning, las estadísticas
individuales estaban manejadas mediante un selector (para seleccionar el
clasificador base) y unos \texttt{checkboxes} para seleccionar las estadísticas
a mostrar. Pero tiene mucho más sentido que el selector sea para las
estadísticas. De esta forma el usuario selecciona una estadística y en el
gráfico puede comparar esa estadística para todos los clasificadores. Además,
los \texttt{checkboxes} se mantienen, pero ahora sirven para elegir qué
clasificadores comparar. Esto llevó unas 6 horas. La organización de las
funciones estaba altamente centrada en la versión anterior, fue un proceso
complicado y lioso.

Pese a que en un principio no se comentó, una vez que se terminó el punto
anterior, se vio necesaria una reestructuración de la página de las
visualizaciones. Se mejoraron las leyendas de tal forma que ya no estaban dentro
de los SVGs, ahora están en su propio cuadro. Esto ha conseguido no preocuparse
por el tamaño de las palabras, centrarla y poder organizar mejor las columnas en
las que se divide esa plantilla.

Sobre el manual del programador, se añadió la estructura de directorios con el
paquete \texttt{dirtree} de \LaTeX, se completó el manual del programador, centrado en
qué es lo que debe saber un desarrollador para continuar con el proyecto y
finalmente se describió el proceso de la compilación, instalación y ejecución
del proyecto. En total fueron unas 6 horas.

El proceso de validación cruzada no estaba bien enfocado, se estaba realizando
de forma manual (y todos los posibles fallos que puede suponer) aunque la
librería scikit-learn incorpora ya utilidades para este proceso. Concretamente
se tiene un método que genera los distintos \texttt{Folds} dado un conjunto de
datos. El proceso manual se sustituyó por este nuevo. Se aprovechó para guardar
los resultados como CSV.

Con el proceso de validación cruzada la idea era obtener métricas para
compararlas en alguna gráfica/tabla y comprobar que las implementaciones son
correctas (comparadas con \texttt{sslearn}). Para ello se creó una función que
recogía la información de los CSVs y dibujaba una malla con distintos gráficos.
En las columnas se distribuyen los algoritmos, separadas en dos para la
implementación propia y la de \texttt{sslearn}. En las filas se tiene cada
estadística. Cada uno de los gráficos de esta malla es un gráfico de cajas (para
mostrar mínimos, máximos, medias, medianas...).

Sobre el último punto, aunque la idea era realizarlo en este Sprint, no se
estimó bien la cantidad de trabajo no pudo ser realizado. Además, en las
ejecuciones del punto anterior (la comparativa) se vio que el algoritmo
Co-Training estaba funcionando por debajo que el de \texttt{sslearn}. Y esto
impedí además justificar en la memoria la validación de los algoritmos.

Durante estas tareas fueron surgiendo pequeños arreglos: Se colorearon las
etiquetas de la ayuda contextual (\texttt{tooltip}) para que quedara claro qué
etiqueta lleva cada punto (no solo el nombre) y se actualizaron las
traducciones.


El gráfico Burndown se visualiza en la imagen~\ref{fig:sprints/sprint11}.
\imagen{sprints/sprint11}{Burndown chart del sprint 11.}


\subsection{Sprint 12}
Corresponde con el periodo temporal del 22 al 29 de marzo de 2023.

Puntos a desarrollar:
\begin{enumerate}
    \item Completar manual del programador.
    \item Introducción memoria.
    \item Añadir bibliotecas Web.
    \item Añadir explicaciones de los algoritmos en la Web.
    \item Añadir los pseudocódigos en la Web.
    \item Algoritmo Tri-Training.
\end{enumerate}

En el Sprint anterior se empezó el manual del programador, pero quedó pendiente
la compilación, instalación y ejecución del proyecto. Lo primero que se hizo en
este Sprint es finalizar este manual describiendo todos los pasos que un
programador debe seguir para poner en funcionamiento la aplicación.

Para continuar con la memoria, se añadió la sección de la \texttt{Introducción}
como presentación de todo este trabajo.

Las dos principales bibliotecas o recursos utilizados en la Web son
\texttt{Bootstrap} y \texttt{D3.js}, el primer caso es un <<framework>> de CSS
que ya viene con una gran cantidad de clases a utilizar. D3 es la biblioteca de
JavaScript que se ha utilizado para la generación de todos los gráficos. Se
añadió una descripción (y justificación) de uso en las \texttt{Técnicas y
herramientas}.

Teniendo en cuanta la componente docente de este proyecto, se creyó conveniente
la adición de unas pequeñas explicaciones de los algoritmos. Se añadieron en la
fase de configuración de los algoritmos dado que al mismo tiempo el usuario debe
configurar los parámetros y puede que le permita tener una mayor intuición a la
hora de configurarlo.

En esta misma línea se añadieron los pseudocódigos en forma de imágenes, justo
debajo de las explicaciones. También se creyó conveniente que el usuario pudiera
verlo en la fase de visualización. Se creó un desplegable con la imagen del
pseudocódigo.

Finalmente, y como desarrollo más grande de este Sprint, se creó el algoritmo
Tri-Training. La primera versión (el primer \texttt{commit}) no fue validada de
ninguna forma (se confirmó según se terminó). Esta versión no incluía ninguna
forma de obtener el proceso de entrenamiento, simplemente era el esqueleto
funcional del algoritmo. Obviamente, era claro que iba a haber algún que otro
error. Al principio solo se detectó un error en el que el cálculo del error de
clasificación siempre resultaba en 0. Era una cuestión de pura implementación en
Python así que no fue difícil arreglarlo.

Antes de arreglar este error, la rama en la que se ejecutaba el <<subsample>> no
se estaba realizando en ningún caso. En el momento que se arregló el anterior
error se accedió a esta parte y apareció un nuevo error. Se confundió array de
NumPy con lista nativa de Python y se estaban seleccionando ciertas posiciones
de una lista de Python accediendo a ella con unos índices ( List{[}indices{]} ).
La solución fue sencilla de igual manera, se recorría la lista en base la los
índices y se creaba esta sublista mediante compresión de listas.

Al final de este desarrollo, se comparó con una sola ejecución con sslearn. El
nuevo algoritmo tardaba mucho más tiempo que el resto (y que sslearn). Esto era
porque al añadir las predicciones (cuando las otras dos hipótesis coincidían) se
hacía de una en una. Esto se sustituyó para predecir todas las instancias no
etiquetadas y con vectorización. El rendimiento se incrementó mucho.

Durante estas tareas fueron surgiendo otras más pequeñas: Nuevas traducciones,
correcciones de memoria y anexos y una pequeña comparación contra sslearn.

La herramienta Zenhub dejó de proporcionar servicios gratuitos. Esta compañía
actualizó sus planes gratuitos, ya no existe una versión de esas
características. Durante este Sprint <<caducó>> la licencia que se tenía y ya no
se podía acceder a ninguna de sus funcionalidades, tampoco a los gráficos
<<Burndown>>.

\subsection{Sprint 13}
Corresponde con el periodo temporal del 29 marzo al 5 de abril de 2023.

Puntos a desarrollar:
\begin{enumerate}
    \item Configuración Tri-Training
    \item Visualizar Tri-Training.
    \item Eliminar todo el código duplicado restante de JavaScript.
    \item Documentar JavaScript.
    \item Reestructurar Flask para aplicaciones grandes.
    \item Añadir media geométrica en la validación/comparación contra sslearn.
\end{enumerate}

Para este Sprint el objetivo principal era dejar lista la visualización de
Tri-Training en la Web. 

El primer paso fue incluir las estructuras de datos necesarias en el esqueleto
del algoritmo para ir almacenando la información de entrenamiento. La idea era
igual a Democratic Co-Learning. La realidad es que la forma en la que se
almacenan los datos es muy distinta a todas las demás. Los datos pueden ser
clasificador varias veces por cada clasificador en diferentes iteraciones. Por
lo tanto, para cada clasificador base se almacena una lista para las etiquetas y
para las iteraciones en las que se clasifican. Las estadísticas sí que se
almacenan exactamente de la misma forma que Democratic Co-Learning.

El siguiente paso fue crear la pantalla de configuración de Tri-Training, para
ello simplemente se copiaron las plantillas ya existentes y la misma estructura
que ya se tenía en Flask para las rutas.

Y finalmente, la pantalla de visualización de Tri-Training. Este fue uno de los
pasos más complejos. Como se tenía una nueva estructura de los datos, se tenían
que crear los métodos que permitiesen interpretarlos, de forma general, los que
se ha realizado es comprobar, en cada iteración, qué clasificador han añadido
datos etiquetados nuevos junto con su etiqueta. Para que el usuario pudiera ver
los cambios que ocurren en cada iteración, cuando pulsa en la siguiente
iteración se descoloran los puntos (a gris) y después se colorean los nuevos,
todo esto mediante animaciones y con un cierto tiempo para que pueda darse
cuenta.

En cuanto a JavaScript, se documentó por completo todos los métodos que fueron
creados para este proyecto. Además, se eliminó el código duplicado en cada
fichero utilizando métodos comunes. También fue un proceso largo, sobre todo para
comprobar que las modificaciones eran correctas y generales, en principio, todo
parece correcto.

Se reestructuró completamente Flask, esto es porque tal y como se encontraba la
aplicación, era muy básica y general. Los proyecto más grandes con Flask tienen
una estructura más o menos concreta. Esta es la idea que se ha intentado
aplicar, utilizando <<Application Facotory>> y <<Blueprints>>. Pero sobre todo,
para hacerla más mantenible y extensible. De hecho, aunque no está siendo usada,
se tiene una pequeña base de datos para posibles adiciones futuras. Esta
reestructuración causó muchos problemas en cuanto a las rutas de ficheros. Se
tuvieron que especificar manualmente y usar la librería del sistema operativo de
Python para independizar ciertos tratamientos de ficheros del sistema operativo
concreto donde corra la aplicación (se ha probado en Windows y Linux).

Finalmente, se añadió la media geométrica como métrica de validación a la hora
de comparar contra sslearn. Además, en Sprints anteriores se comentó la idea de
utilizar \texttt{Violinplots}, que resultan más pertinentes para los casos de
comparación.

\subsection{Sprint 14}
Corresponde con el periodo temporal del 5 al 12 de abril de 2023.

Puntos a desarrollar:
\begin{enumerate}
    \item Actualizar manual del programador
    \item Rediseño Web completo
    \item Ayuda contextual y errores en Web.
    \item Anexos: Diseño
\end{enumerate}

Debido a las modificaciones realizadas en los recientes Sprints, el manual del
programador quedó desactualizado. El cambio más notable fue la nueva
reestructuración de Flask. Se actualizó el árbol de directorios reflejando esta
nueva estructura. También se actualizaron todas las explicaciones posteriores
del manual.

El diseño de la Web estaba hecho prácticamente desde el principio de la Web y
resultaba muy poco llamativo. El primer paso para este rediseño fue elegir una
paleta de colores, se creyeron convenientes fondos oscuros con alguno más claro
para dar contraste.

Para continuar, se pensó en la disposición y estilo general del contenido. Hasta
el momento la página resultaba muy plana, sin cambios entre las distintas partes
de la página. Consultando páginas Web de ejemplo parecía interesante y bonito
crear efectos de sombra y se estableció como estilo general de la página. A
partir de aquí las distintas secciones de una misma página se encuadran en un
elemento con sombra, para llamar la atención del usuario. Además, para comprobar
que no eran ideas descabelladas, se consultó a compañeros y personas externas
para buscar opinión. Gracias a esto se realizaron retoques (cambio de algún
color, títulos, disposiciones...) que no convencía a la mayoría.


Para proporcionar ayuda básica en las distintas páginas, se creó una macro que
genera una \texttt{tooltip} con un mensaje. Tanto en la subida como en la
configuración se utilizaron estos mensajes para aclarar aspectos confusos. Como
adición, resultaba muy molesto tener que volver a subir un fichero
constantemente al seleccionar otro algoritmo durante la misma sesión. Se añadió
una comprobación en la pestaña de subida en la que si ya se había subido un
fichero, permitiera ir directamente a la configuración.

Durante todo el desarrollo hasta este momento, ocurrían ciertos errores HTTP
(404, por ejemplo) pero no eran controlados. Se creó una plantilla base y se le
indicó a Flask los errores que debía manejar y renderizar en esa plantilla. Se
han añadido los más comunes (y además muchos de ellos no han llegado a ocurrir
nunca).

En reuniones anteriores se comentó que debía explicarse las estructuras de los
ficheros JSON que maneja e interpreta la Web (JavaScript). Se añadieron todas
las explicaciones relativas a la generación de estos ficheros desde la propia
ejecución de cada algoritmo. Se creó también un diagrama de secuencia con la
interacción general con la Web para la visualización de un algoritmo.

\subsection{Sprint 14}
Corresponde con el periodo temporal del 12 al 19 de abril de 2023.

Durante este Sprint tuvo lugar el periodo de exámenes parciales de las
asignaturas cuatrimestrales, y junto con las prácticas en empresa, no se pudo
mantener la prioridad al desarrollo del proyecto. Por lo tanto, no existen
puntos concretos a desarrollar, pero la idea era realizar, en la medida de lo
posible, retoques en el mismo.

Los \texttt{violinplots} que se añadieron para la comparativa de algoritmos
resultaban algo confusos. Aunque este tipo de gráficos se utiliza mucho para
comparar datos, para este ámbito concreto no resultó ser la mejor opción. La
parte superior de estos gráficos, que no son el máximo de los datos, sobrepasa
en algunos casos el valor máximo de las estadísticas (1). Esto hace parecer que
los cálculos son incorrectos. Se volvió a incluir los gráficos de caja
anteriores.

Como no resultaba una tarea grande, se incluyó la integración continua a partir
de este momento. La herramienta SonarCloud proporciona muchas métricas de
calidad del código (bugs, línea duplicadas, seguridad...). Gracias a estos
análisis se realizaron todas las posibles modificaciones para reducir las
alertas que hasta el momento podían abordarse. Por ejemplo, la protección contra
CSRF (\texttt{Cross-Site Request Forgery}) no se había contemplado, pero la
herramienta sí lo considera como un elemento prioritario.

\section{Estudio de viabilidad}

\subsection{Viabilidad económica}

\subsection{Viabilidad legal}


